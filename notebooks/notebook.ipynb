{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3432a6",
   "metadata": {},
   "source": [
    "livree par client employee et date meme chose avec non livreee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11244d89",
   "metadata": {},
   "source": [
    "# Livree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b30626",
   "metadata": {},
   "source": [
    "## Command + client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d1035f",
   "metadata": {},
   "source": [
    "# Resultat + employee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76941f1",
   "metadata": {},
   "source": [
    "# non Livree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff66b70",
   "metadata": {},
   "source": [
    "# model pour represente un entropot projet te3na ndiro etoile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd056f39",
   "metadata": {},
   "source": [
    "Etoile / constelation / flocan de naige \n",
    "Table de fait / Table de dimention\n",
    "1/etile ( une seul table de fait central - pas de relation entre les tables de dimentions mais il ya entre fait et dimention)\n",
    "pas de relation entre les dimentions alors abscence de hierachie entre les dim\n",
    "\n",
    "etoile (simple) ---> flocan (une seul table de fait +dimention seoare+heiarchi entre dim +normalisation) ---> constelation(redondance plusieur fait dim commune entre fait+hearchie entre dimention) ( koul wa7ed rigla lo5er)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bdeb6b",
   "metadata": {},
   "source": [
    "mesure = client , ordre , employee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b1119",
   "metadata": {},
   "source": [
    "dim (axe d'analise) = client , ordre , employee,temps (lazem dire dim temps )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c466700e",
   "metadata": {},
   "source": [
    "multi dimention = donne de plusieur tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3806472b",
   "metadata": {},
   "source": [
    "Fait te3na hya command 3endha id howa id sequenciel ( num qui incremante) + concatination key primaire des dimentions id1id2...\n",
    "+ attribute te3 command nbr commandes livree/non livree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daead03",
   "metadata": {},
   "source": [
    "# Cleaning ACCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae51dfb",
   "metadata": {},
   "source": [
    "- **Customers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201f7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [customers,order_details,orders,products,purchase_orders,shippers,suppliers]:\n",
    "        i.drop_duplicates(inplace=True)\n",
    "\n",
    "#['Order Date'] = i['Order Date'].str.strip()\n",
    "\n",
    "customers['ContactName'] = (\n",
    "    customers['First Name'].fillna('') + ' ' + customers['Last Name'].fillna('')\n",
    ")\n",
    "customers['ContactName'] = customers['ContactName'].str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
    "customers.drop(columns=['First Name', 'Last Name'], inplace=True)\n",
    "\n",
    "customers.rename(columns={'Country/Region':'Country'},inplace=True)\n",
    "customers.drop(columns=['E-mail Address','Job Title','Business Phone','Home Phone','Mobile Phone',\n",
    "                        'Fax Number','Address','City','State/Province','ZIP/Postal Code','Web Page',\n",
    "                        'Notes','Attachments'],inplace=True)\n",
    "\n",
    "customers.to_csv('customer.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb9f90",
   "metadata": {},
   "source": [
    "- **orders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3738c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders['Order Date'] = pd.to_datetime(orders['Order Date']).dt.strftime('%Y-%m-%d')\n",
    "orders['Shipped Date'] = pd.to_datetime(orders['Shipped Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "cols_to_drop = [\n",
    "    'Ship Name', 'Ship Address', 'Ship City', 'Ship State/Province',\n",
    "    'Ship ZIP/Postal Code', 'Ship Country/Region',\n",
    "    'Taxes', 'Payment Type', 'Paid Date',\n",
    "    'Notes', 'Tax Rate', 'Tax Status', 'Status ID'\n",
    "]\n",
    "orders.drop(columns=[c for c in cols_to_drop if c in orders.columns], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "orders.columns = orders.columns.str.replace(' ','',regex = False)\n",
    "\n",
    "orders.to_csv('order.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e60d4d2",
   "metadata": {},
   "source": [
    "- **Employee**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d0257",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'First Name' in employees.columns or 'Last Name' in employees.columns:\n",
    "    employees['FullName'] = (\n",
    "        employees.get('First Name', '').fillna('') + ' ' + employees.get('Last Name', '').fillna('')\n",
    "    ).str.strip()\n",
    "    # drop the original name columns if they exist\n",
    "    employees.drop(columns=[c for c in ['First Name', 'Last Name'] if c in employees.columns], inplace=True)\n",
    "\n",
    "cols_to_remove = [\n",
    "    'E-mail Address','Job Title','Business Phone','Home Phone','Mobile Phone',\n",
    "    'Fax Number','Address','City','State/Province','ZIP/Postal Code',\n",
    "    'Country/Region','Web Page','Notes','Attachments'\n",
    "]\n",
    "employees.drop(columns=[c for c in cols_to_remove if c in employees.columns], inplace=True)\n",
    "\n",
    "employees.to_csv('employees.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a42b2b",
   "metadata": {},
   "source": [
    "# Cleaning SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e546a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers.rename(columns={'CompanyName':'Company'},inplace=True)\n",
    "cols_to_drop = [\n",
    "    'ContactTitle','Address','City','Region','PostalCode','Phone','Fax'\n",
    "]\n",
    "df_customers.drop(columns=[c for c in cols_to_drop if c in df_customers.columns], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf6e3a",
   "metadata": {},
   "source": [
    "# ANALYTICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9614fc",
   "metadata": {},
   "source": [
    "## 1- TRANSFORM WAREHOUSE INTO CSV FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0eae33f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(pyodbc.OperationalError) ('08001', '[08001] [Microsoft][ODBC Driver 17 for SQL Server]SQL Server Network Interfaces: Error Locating Server/Instance Specified [xFFFFFFFF].  (-1) (SQLDriverConnect); [08001] [Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired (0); [08001] [Microsoft][ODBC Driver 17 for SQL Server]A network-related or instance-specific error has occurred while establishing a connection to SQL Server. Server is not found or not accessible. Check if instance name is correct and if SQL Server is configured to allow remote connections. For more information see SQL Server Books Online. (-1)')\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:143\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28mself\u001b[39m._dbapi_connection = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3301\u001b[39m, in \u001b[36mEngine.raw_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3280\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[32m   3281\u001b[39m \n\u001b[32m   3282\u001b[39m \u001b[33;03mThe returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3299\u001b[39m \n\u001b[32m   3300\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3301\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:447\u001b[39m, in \u001b[36mPool.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[32m    441\u001b[39m \n\u001b[32m    442\u001b[39m \u001b[33;03mThe connection is instrumented such that when its\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    445\u001b[39m \n\u001b[32m    446\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:1264\u001b[39m, in \u001b[36m_ConnectionFairy._checkout\u001b[39m\u001b[34m(cls, pool, threadconns, fairy)\u001b[39m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     fairy = \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1266\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:711\u001b[39m, in \u001b[36m_ConnectionRecord.checkout\u001b[39m\u001b[34m(cls, pool)\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m     rec = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:177\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dec_overflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:175\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:388\u001b[39m, in \u001b[36mPool._create_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:673\u001b[39m, in \u001b[36m_ConnectionRecord.__init__\u001b[39m\u001b[34m(self, pool, connect)\u001b[39m\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[38;5;28mself\u001b[39m.finalize_callback = deque()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:899\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m899\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mError on connect(): \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:895\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    894\u001b[39m \u001b[38;5;28mself\u001b[39m.starttime = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m895\u001b[39m \u001b[38;5;28mself\u001b[39m.dbapi_connection = connection = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    896\u001b[39m pool.logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:661\u001b[39m, in \u001b[36mcreate_engine.<locals>.connect\u001b[39m\u001b[34m(connection_record)\u001b[39m\n\u001b[32m    659\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:629\u001b[39m, in \u001b[36mDefaultDialect.connect\u001b[39m\u001b[34m(self, *cargs, **cparams)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cargs: Any, **cparams: Any) -> DBAPIConnection:\n\u001b[32m    628\u001b[39m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloaded_dbapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOperationalError\u001b[39m: ('08001', '[08001] [Microsoft][ODBC Driver 17 for SQL Server]SQL Server Network Interfaces: Error Locating Server/Instance Specified [xFFFFFFFF].  (-1) (SQLDriverConnect); [08001] [Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired (0); [08001] [Microsoft][ODBC Driver 17 for SQL Server]A network-related or instance-specific error has occurred while establishing a connection to SQL Server. Server is not found or not accessible. Check if instance name is correct and if SQL Server is configured to allow remote connections. For more information see SQL Server Books Online. (-1)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      6\u001b[39m params = urllib.parse.quote_plus(\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDriver=\u001b[39m\u001b[33m{\u001b[39m\u001b[33mODBC Driver 17 for SQL Server};\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mServer=localhost\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mSQLEXPRESS;\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDatabase=Northwind_DW;\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTrusted_Connection=yes;\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m engine = create_engine(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmssql+pyodbc:///?odbc_connect=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m dim_customer = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSELECT * FROM DimCustomer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m dim_employee = pd.read_sql(\u001b[33m\"\u001b[39m\u001b[33mSELECT * FROM DimEmployee\u001b[39m\u001b[33m\"\u001b[39m, engine)\n\u001b[32m     18\u001b[39m dim_time = pd.read_sql(\u001b[33m\"\u001b[39m\u001b[33mSELECT * FROM DimTime\u001b[39m\u001b[33m\"\u001b[39m, engine)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\pandas\\io\\sql.py:706\u001b[39m, in \u001b[36mread_sql\u001b[39m\u001b[34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[39m\n\u001b[32m    703\u001b[39m     dtype_backend = \u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpandasSQL_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcon\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[32m    708\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql.read_query(\n\u001b[32m    709\u001b[39m             sql,\n\u001b[32m    710\u001b[39m             index_col=index_col,\n\u001b[32m   (...)\u001b[39m\u001b[32m    716\u001b[39m             dtype=dtype,\n\u001b[32m    717\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\pandas\\io\\sql.py:908\u001b[39m, in \u001b[36mpandasSQL_builder\u001b[39m\u001b[34m(con, schema, need_transaction)\u001b[39m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUsing URI string without sqlalchemy installed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    907\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sqlalchemy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(con, (\u001b[38;5;28mstr\u001b[39m, sqlalchemy.engine.Connectable)):\n\u001b[32m--> \u001b[39m\u001b[32m908\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSQLDatabase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_transaction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    910\u001b[39m adbc = import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33madbc_driver_manager.dbapi\u001b[39m\u001b[33m\"\u001b[39m, errors=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    911\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m adbc \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(con, adbc.Connection):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\pandas\\io\\sql.py:1648\u001b[39m, in \u001b[36mSQLDatabase.__init__\u001b[39m\u001b[34m(self, con, schema, need_transaction)\u001b[39m\n\u001b[32m   1646\u001b[39m     \u001b[38;5;28mself\u001b[39m.exit_stack.callback(con.dispose)\n\u001b[32m   1647\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(con, Engine):\n\u001b[32m-> \u001b[39m\u001b[32m1648\u001b[39m     con = \u001b[38;5;28mself\u001b[39m.exit_stack.enter_context(\u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1649\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m need_transaction \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m con.in_transaction():\n\u001b[32m   1650\u001b[39m     \u001b[38;5;28mself\u001b[39m.exit_stack.enter_context(con.begin())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3277\u001b[39m, in \u001b[36mEngine.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3254\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Connection:\n\u001b[32m   3255\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a new :class:`_engine.Connection` object.\u001b[39;00m\n\u001b[32m   3256\u001b[39m \n\u001b[32m   3257\u001b[39m \u001b[33;03m    The :class:`_engine.Connection` acts as a Python context manager, so\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3274\u001b[39m \n\u001b[32m   3275\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:145\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    143\u001b[39m         \u001b[38;5;28mself\u001b[39m._dbapi_connection = engine.raw_connection()\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m         \u001b[43mConnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle_dbapi_exception_noconnection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2440\u001b[39m, in \u001b[36mConnection._handle_dbapi_exception_noconnection\u001b[39m\u001b[34m(cls, e, dialect, engine, is_disconnect, invalidate_pool_on_disconnect, is_pre_ping)\u001b[39m\n\u001b[32m   2438\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[32m   2439\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2440\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception.with_traceback(exc_info[\u001b[32m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2441\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2442\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[32m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:143\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m         \u001b[38;5;28mself\u001b[39m._dbapi_connection = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    145\u001b[39m         Connection._handle_dbapi_exception_noconnection(\n\u001b[32m    146\u001b[39m             err, dialect, engine\n\u001b[32m    147\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3301\u001b[39m, in \u001b[36mEngine.raw_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3279\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraw_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> PoolProxiedConnection:\n\u001b[32m   3280\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[32m   3281\u001b[39m \n\u001b[32m   3282\u001b[39m \u001b[33;03m    The returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3299\u001b[39m \n\u001b[32m   3300\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:447\u001b[39m, in \u001b[36mPool.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> PoolProxiedConnection:\n\u001b[32m    440\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[32m    441\u001b[39m \n\u001b[32m    442\u001b[39m \u001b[33;03m    The connection is instrumented such that when its\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    445\u001b[39m \n\u001b[32m    446\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:1264\u001b[39m, in \u001b[36m_ConnectionFairy._checkout\u001b[39m\u001b[34m(cls, pool, threadconns, fairy)\u001b[39m\n\u001b[32m   1256\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_checkout\u001b[39m(\n\u001b[32m   1258\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1261\u001b[39m     fairy: Optional[_ConnectionFairy] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1262\u001b[39m ) -> _ConnectionFairy:\n\u001b[32m   1263\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m         fairy = \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1266\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1267\u001b[39m             threadconns.current = weakref.ref(fairy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:711\u001b[39m, in \u001b[36m_ConnectionRecord.checkout\u001b[39m\u001b[34m(cls, pool)\u001b[39m\n\u001b[32m    709\u001b[39m     rec = cast(_ConnectionRecord, pool._do_get())\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m     rec = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    714\u001b[39m     dbapi_connection = rec.get_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:177\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_connection()\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dec_overflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    226\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:175\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inc_overflow():\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    177\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m util.safe_reraise():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:388\u001b[39m, in \u001b[36mPool._create_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ConnectionPoolEntry:\n\u001b[32m    386\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:673\u001b[39m, in \u001b[36m_ConnectionRecord.__init__\u001b[39m\u001b[34m(self, pool, connect)\u001b[39m\n\u001b[32m    671\u001b[39m \u001b[38;5;28mself\u001b[39m.__pool = pool\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[38;5;28mself\u001b[39m.finalize_callback = deque()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:899\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    897\u001b[39m     \u001b[38;5;28mself\u001b[39m.fresh = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m899\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mError on connect(): \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    902\u001b[39m     \u001b[38;5;66;03m# in SQLAlchemy 1.4 the first_connect event is not used by\u001b[39;00m\n\u001b[32m    903\u001b[39m     \u001b[38;5;66;03m# the engine, so this will usually not be set\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    226\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:895\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    894\u001b[39m     \u001b[38;5;28mself\u001b[39m.starttime = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m895\u001b[39m     \u001b[38;5;28mself\u001b[39m.dbapi_connection = connection = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    896\u001b[39m     pool.logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, connection)\n\u001b[32m    897\u001b[39m     \u001b[38;5;28mself\u001b[39m.fresh = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:661\u001b[39m, in \u001b[36mcreate_engine.<locals>.connect\u001b[39m\u001b[34m(connection_record)\u001b[39m\n\u001b[32m    658\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    659\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Documents\\BI PRJ\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:629\u001b[39m, in \u001b[36mDefaultDialect.connect\u001b[39m\u001b[34m(self, *cargs, **cparams)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cargs: Any, **cparams: Any) -> DBAPIConnection:\n\u001b[32m    628\u001b[39m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloaded_dbapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOperationalError\u001b[39m: (pyodbc.OperationalError) ('08001', '[08001] [Microsoft][ODBC Driver 17 for SQL Server]SQL Server Network Interfaces: Error Locating Server/Instance Specified [xFFFFFFFF].  (-1) (SQLDriverConnect); [08001] [Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired (0); [08001] [Microsoft][ODBC Driver 17 for SQL Server]A network-related or instance-specific error has occurred while establishing a connection to SQL Server. Server is not found or not accessible. Check if instance name is correct and if SQL Server is configured to allow remote connections. For more information see SQL Server Books Online. (-1)')\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "\n",
    "# Encode the ODBC driver name\n",
    "params = urllib.parse.quote_plus(\n",
    "    \"Driver={ODBC Driver 17 for SQL Server};\"\n",
    "    \"Server=localhost\\\\SQLEXPRESS;\"\n",
    "    \"Database=Northwind_DW;\"\n",
    "    \"Trusted_Connection=yes;\"\n",
    ")\n",
    "\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "\n",
    "dim_customer = pd.read_sql(\"SELECT * FROM DimCustomer\", engine)\n",
    "dim_employee = pd.read_sql(\"SELECT * FROM DimEmployee\", engine)\n",
    "dim_time = pd.read_sql(\"SELECT * FROM DimTime\", engine)\n",
    "order_fact = pd.read_sql(\"SELECT * FROM Order_Fact\", engine)\n",
    "\n",
    "# Export to CSV\n",
    "dim_customer.to_csv(\"../Data/warehouse/DimCustomer.csv\", index=False)\n",
    "dim_employee.to_csv(\"../Data/warehouse/DimEmployee.csv\", index=False)\n",
    "dim_time.to_csv(\"../Data/warehouse/DimTime.csv\", index=False)\n",
    "order_fact.to_csv(\"../Data/warehouse/Order_Fact.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aaf8bc",
   "metadata": {},
   "source": [
    "## 2- TRANSFORM THE WAREHOUSE CSV FILES INTO SPECIFIC ANALYTICS / DASHBOARDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3267055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "customers = pd.read_csv(\"../Data/warehouse/DimCustomer.csv\")\n",
    "employees = pd.read_csv(\"../Data/warehouse/DimEmployee.csv\")\n",
    "orders = pd.read_csv(\"../Data/warehouse/Order_Fact.csv\")\n",
    "time_dim = pd.read_csv(\"../Data/warehouse/DimTime.csv\")\n",
    "\n",
    "customers.columns = customers.columns.str.strip()\n",
    "employees.columns = employees.columns.str.strip()\n",
    "orders.columns = orders.columns.str.strip()\n",
    "time_dim.columns = time_dim.columns.str.strip()\n",
    "\n",
    "orders['ShippedDateKey'] = orders['ShippedDateKey'].astype(int)\n",
    "\n",
    "shipped_summary = pd.DataFrame({\n",
    "    'ShippedStatus': ['Shipped', 'NotShipped'],\n",
    "    'Count': [\n",
    "        (orders['ShippedDateKey'] != 1011900).sum(),\n",
    "        (orders['ShippedDateKey'] == 1011900).sum()\n",
    "    ]\n",
    "})\n",
    "shipped_summary.to_csv(\"../Data/analytics/shipped_summary.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "shipped_by_employee = orders[orders['ShippedDateKey'] != 1011900]\\\n",
    "    .groupby('EmployeeID').size().reset_index(name='ShippedCount')\n",
    "shipped_by_employee = shipped_by_employee.merge(employees[['EmployeeID','FullName']], on='EmployeeID')\n",
    "shipped_by_employee.to_csv(\"../Data/analytics/shipped_by_employee.csv\", index=False)\n",
    "\n",
    "\n",
    "shipped_by_customer = orders[orders['ShippedDateKey'] != 1011900]\\\n",
    "    .groupby('CustomerID').size().reset_index(name='ShippedCount')\n",
    "shipped_by_customer = shipped_by_customer.merge(customers[['CustomerID','Company']], on='CustomerID')\n",
    "shipped_by_customer.to_csv(\"../Data/analytics/shipped_by_customer.csv\", index=False)\n",
    "\n",
    "shipped_by_country = orders[orders['ShippedDateKey'] != 1011900]\\\n",
    "    .merge(customers[['CustomerID','Country']], on='CustomerID')\\\n",
    "    .groupby('Country').size().reset_index(name='ShippedCount')\n",
    "shipped_by_country.to_csv(\"../Data/analytics/shipped_by_country.csv\", index=False)\n",
    "\n",
    "shipped_by_company = orders[orders['ShippedDateKey'] != 1011900]\\\n",
    "    .merge(customers[['CustomerID','Company']], on='CustomerID')\\\n",
    "    .groupby('Company').size().reset_index(name='ShippedCount')\n",
    "shipped_by_company.to_csv(\"../Data/analytics/shipped_by_company.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Not shipped by employee\n",
    "not_shipped_by_employee = orders[orders['ShippedDateKey'] == 1011900]\\\n",
    "    .groupby('EmployeeID').size().reset_index(name='NotShippedCount')\n",
    "not_shipped_by_employee = not_shipped_by_employee.merge(employees[['EmployeeID','FullName']], on='EmployeeID')\n",
    "not_shipped_by_employee.to_csv(\"../Data/analytics/not_shipped_by_employee.csv\", index=False)\n",
    "\n",
    "# Not shipped by customer\n",
    "not_shipped_by_customer = orders[orders['ShippedDateKey'] == 1011900]\\\n",
    "    .groupby('CustomerID').size().reset_index(name='NotShippedCount')\n",
    "not_shipped_by_customer = not_shipped_by_customer.merge(customers[['CustomerID','Company']], on='CustomerID')\n",
    "not_shipped_by_customer.to_csv(\"../Data/analytics/not_shipped_by_customer.csv\", index=False)\n",
    "\n",
    "# Not shipped by country\n",
    "not_shipped_by_country = orders[orders['ShippedDateKey'] == 1011900]\\\n",
    "    .merge(customers[['CustomerID','Country']], on='CustomerID')\\\n",
    "    .groupby('Country').size().reset_index(name='NotShippedCount')\n",
    "not_shipped_by_country.to_csv(\"../Data/analytics/not_shipped_by_country.csv\", index=False)\n",
    "\n",
    "# Not shipped by company\n",
    "not_shipped_by_company = orders[orders['ShippedDateKey'] == 1011900]\\\n",
    "    .merge(customers[['CustomerID','Company']], on='CustomerID')\\\n",
    "    .groupby('Company').size().reset_index(name='NotShippedCount')\n",
    "not_shipped_by_company.to_csv(\"../Data/analytics/not_shipped_by_company.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "shipped_orders = orders[orders['ShippedDateKey'] != 1011900]\n",
    "\n",
    "# Merge with DimTime on ShippedDateKey\n",
    "shipped_orders = shipped_orders.merge(\n",
    "    time_dim[['DateKey', 'Year', 'Month']], \n",
    "    left_on='ShippedDateKey', \n",
    "    right_on='DateKey', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "shipped_per_year = shipped_orders.groupby('Year').size().reset_index(name='ShippedCount')\n",
    "shipped_per_year = shipped_per_year.sort_values(by='ShippedCount', ascending=False)\n",
    "shipped_per_year.to_csv(\"../Data/analytics/most_shipped_per_year.csv\", index=False)\n",
    "\n",
    "# Most shipped per month (across all years)\n",
    "shipped_per_month = shipped_orders.groupby('Month').size().reset_index(name='ShippedCount')\n",
    "shipped_per_month = shipped_per_month.sort_values(by='ShippedCount', ascending=False)\n",
    "shipped_per_month.to_csv(\"../Data/analytics/most_shipped_per_month.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4f82a9",
   "metadata": {},
   "source": [
    "# Regenerate Analytics Files\n",
    "\n",
    "Run this cell to update all analytics CSV files with the corrected data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a002f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reload warehouse data\n",
    "customers = pd.read_csv(\"../Data/warehouse/DimCustomer.csv\")\n",
    "employees = pd.read_csv(\"../Data/warehouse/DimEmployee.csv\")\n",
    "orders = pd.read_csv(\"../Data/warehouse/Order_Fact.csv\")\n",
    "time_dim = pd.read_csv(\"../Data/warehouse/DimTime.csv\")\n",
    "\n",
    "# Strip column names\n",
    "customers.columns = customers.columns.str.strip()\n",
    "employees.columns = employees.columns.str.strip()\n",
    "orders.columns = orders.columns.str.strip()\n",
    "time_dim.columns = time_dim.columns.str.strip()\n",
    "\n",
    "orders['ShippedDateKey'] = orders['ShippedDateKey'].astype(int)\n",
    "\n",
    "# Regenerate all analytics files\n",
    "print(\"Regenerating analytics files with both Access and SQL Server data...\")\n",
    "\n",
    "# Shipped summary\n",
    "shipped_summary = pd.DataFrame({\n",
    "    'ShippedStatus': ['Shipped', 'NotShipped'],\n",
    "    'Count': [\n",
    "        (orders['ShippedDateKey'] != 1011900).sum(),\n",
    "        (orders['ShippedDateKey'] == 1011900).sum()\n",
    "    ]\n",
    "})\n",
    "shipped_summary.to_csv(\"../Data/analytics/shipped_summary.csv\", index=False)\n",
    "print(f\" shipped_summary.csv: {len(shipped_summary)} rows\")\n",
    "\n",
    "# Shipped by employee\n",
    "shipped_by_employee = orders[orders['ShippedDateKey'] != 1011900]\\\n",
    "    .groupby('EmployeeID').size().reset_index(name='ShippedCount')\n",
    "shipped_by_employee = shipped_by_employee.merge(employees[['EmployeeID','FullName']], on='EmployeeID')\n",
    "shipped_by_employee.to_csv(\"../Data/analytics/shipped_by_employee.csv\", index=False)\n",
    "print(f\" shipped_by_employee.csv: {len(shipped_by_employee)} rows\")\n",
    "\n",
    "# Shipped by customer\n",
    "shipped_by_customer = orders[orders['ShippedDateKey'] != 1011900]\\\n",
    "    .groupby('CustomerID').size().reset_index(name='ShippedCount')\n",
    "shipped_by_customer = shipped_by_customer.merge(customers[['CustomerID','Company']], on='CustomerID')\n",
    "shipped_by_customer.to_csv(\"../Data/analytics/shipped_by_customer.csv\", index=False)\n",
    "print(f\" shipped_by_customer.csv: {len(shipped_by_customer)} rows\")\n",
    "\n",
    "# Shipped by country\n",
    "shipped_by_country = orders[orders['ShippedDateKey'] != 1011900]\\\n",
    "    .merge(customers[['CustomerID','Country']], on='CustomerID')\\\n",
    "    .groupby('Country').size().reset_index(name='ShippedCount')\n",
    "shipped_by_country.to_csv(\"../Data/analytics/shipped_by_country.csv\", index=False)\n",
    "print(f\" shipped_by_country.csv: {len(shipped_by_country)} rows\")\n",
    "\n",
    "# Shipped by company\n",
    "shipped_by_company = orders[orders['ShippedDateKey'] != 1011900]\\\n",
    "    .merge(customers[['CustomerID','Company']], on='CustomerID')\\\n",
    "    .groupby('Company').size().reset_index(name='ShippedCount')\n",
    "shipped_by_company.to_csv(\"../Data/analytics/shipped_by_company.csv\", index=False)\n",
    "print(f\" shipped_by_company.csv: {len(shipped_by_company)} rows\")\n",
    "\n",
    "# Not shipped by employee\n",
    "not_shipped_by_employee = orders[orders['ShippedDateKey'] == 1011900]\\\n",
    "    .groupby('EmployeeID').size().reset_index(name='NotShippedCount')\n",
    "not_shipped_by_employee = not_shipped_by_employee.merge(employees[['EmployeeID','FullName']], on='EmployeeID')\n",
    "not_shipped_by_employee.to_csv(\"../Data/analytics/not_shipped_by_employee.csv\", index=False)\n",
    "print(f\" not_shipped_by_employee.csv: {len(not_shipped_by_employee)} rows\")\n",
    "\n",
    "# Not shipped by customer\n",
    "not_shipped_by_customer = orders[orders['ShippedDateKey'] == 1011900]\\\n",
    "    .groupby('CustomerID').size().reset_index(name='NotShippedCount')\n",
    "not_shipped_by_customer = not_shipped_by_customer.merge(customers[['CustomerID','Company']], on='CustomerID')\n",
    "not_shipped_by_customer.to_csv(\"../Data/analytics/not_shipped_by_customer.csv\", index=False)\n",
    "print(f\" not_shipped_by_customer.csv: {len(not_shipped_by_customer)} rows\")\n",
    "\n",
    "# Not shipped by country\n",
    "not_shipped_by_country = orders[orders['ShippedDateKey'] == 1011900]\\\n",
    "    .merge(customers[['CustomerID','Country']], on='CustomerID')\\\n",
    "    .groupby('Country').size().reset_index(name='NotShippedCount')\n",
    "not_shipped_by_country.to_csv(\"../Data/analytics/not_shipped_by_country.csv\", index=False)\n",
    "print(f\" not_shipped_by_country.csv: {len(not_shipped_by_country)} rows\")\n",
    "\n",
    "# Not shipped by company\n",
    "not_shipped_by_company = orders[orders['ShippedDateKey'] == 1011900]\\\n",
    "    .merge(customers[['CustomerID','Company']], on='CustomerID')\\\n",
    "    .groupby('Company').size().reset_index(name='NotShippedCount')\n",
    "not_shipped_by_company.to_csv(\"../Data/analytics/not_shipped_by_company.csv\", index=False)\n",
    "print(f\" not_shipped_by_company.csv: {len(not_shipped_by_company)} rows\")\n",
    "print(f\"\\nSample not shipped companies:\\n{not_shipped_by_company.head(10)}\")\n",
    "\n",
    "# Time-based analytics\n",
    "shipped_orders = orders[orders['ShippedDateKey'] != 1011900]\n",
    "shipped_orders = shipped_orders.merge(\n",
    "    time_dim[['DateKey', 'Year', 'Month']], \n",
    "    left_on='ShippedDateKey', \n",
    "    right_on='DateKey', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "shipped_per_year = shipped_orders.groupby('Year').size().reset_index(name='ShippedCount')\n",
    "shipped_per_year = shipped_per_year.sort_values(by='ShippedCount', ascending=False)\n",
    "shipped_per_year.to_csv(\"../Data/analytics/most_shipped_per_year.csv\", index=False)\n",
    "print(f\" most_shipped_per_year.csv: {len(shipped_per_year)} rows\")\n",
    "\n",
    "shipped_per_month = shipped_orders.groupby('Month').size().reset_index(name='ShippedCount')\n",
    "shipped_per_month = shipped_per_month.sort_values(by='ShippedCount', ascending=False)\n",
    "shipped_per_month.to_csv(\"../Data/analytics/most_shipped_per_month.csv\", index=False)\n",
    "print(f\" most_shipped_per_month.csv: {len(shipped_per_month)} rows\")\n",
    "\n",
    "print(\"\\n All analytics files regenerated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b5a49",
   "metadata": {},
   "source": [
    "#  Projet Business Intelligence : Analyse Northwind Data Warehouse\n",
    "\n",
    "Ce Notebook Jupyter utilise les donnes charges dans le Data Warehouse `NorthwindDW` (via le script `etl.py`) pour gnrer les analyses cls.\n",
    "\n",
    "**Outils utiliss :**\n",
    "* **Connexion :** `pyodbc`\n",
    "* **Manipulation :** `pandas`\n",
    "* **Visualisation :** `matplotlib` et `seaborn`\n",
    "\n",
    "---\n",
    "## 1. Initialisation et Connexion\n",
    "\n",
    "La premire tape consiste  importer les bibliothques ncessaires et  tablir une connexion scurise  l'instance SQL Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550ef850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration de la connexion au Data Warehouse\n",
    "# (Utilise les paramtres identifis dans le script ETL)\n",
    "SQL_DW_SERVER = r'DESKTOP-F8N2M8C\\SQLEXPRESS'\n",
    "SQL_DW_DATABASE = 'NorthwindDW'\n",
    "SQL_DW_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "# Chane de connexion pour l'authentification Windows\n",
    "SQL_CONN_STRING = (\n",
    "    f'DRIVER={SQL_DW_DRIVER};'\n",
    "    f'SERVER={SQL_DW_SERVER};'\n",
    "    f'DATABASE={SQL_DW_DATABASE};'\n",
    "    r'Trusted_Connection=yes;'\n",
    "    r'TrustServerCertificate=yes;'\n",
    ")\n",
    "\n",
    "print(f\"Tentative de connexion au Data Warehouse: {SQL_DW_DATABASE}...\")\n",
    "try:\n",
    "    conn = pyodbc.connect(SQL_CONN_STRING)\n",
    "    print(\" Connexion russie. Le Data Warehouse est prt pour l'analyse.\")\n",
    "except pyodbc.Error as e:\n",
    "    print(f\" chec de la connexion. Vrifiez le serveur et le pilote : {e}\")\n",
    "    conn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458964ee",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Analyse 1 : Tendance des Ventes sur la Priode\n",
    "\n",
    "**Objectif :** valuer la performance globale des ventes (`SalesAmount`) mois par mois afin d'identifier toute croissance, stagnation ou dclin.\n",
    "\n",
    "### **Conclusion :**\n",
    "Le graphique linaire rvle une **tendance gnrale  la baisse** des revenus sur la priode. Le pic initial est suivi d'une rosion progressive, signalant une saturation possible du march ou un besoin de rvaluer les stratgies commerciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d459fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if conn:\n",
    "    # Requte: Tendance des ventes mensuelles (utilise FactSales et DimDate)\n",
    "    query_sales_trend = \"\"\"\n",
    "    SELECT \n",
    "        DD.Year,\n",
    "        DD.Month,\n",
    "        SUM(FS.SalesAmount) AS MonthlySales\n",
    "    FROM \n",
    "        FactSales FS\n",
    "    JOIN \n",
    "        DimDate DD ON FS.OrderDateKey = DD.DateKey\n",
    "    GROUP BY \n",
    "        DD.Year,\n",
    "        DD.Month\n",
    "    ORDER BY \n",
    "        DD.Year,\n",
    "        DD.Month;\n",
    "    \"\"\"\n",
    "\n",
    "    df_sales_trend = pd.read_sql(query_sales_trend, conn)\n",
    "    \n",
    "    # Cration de la colonne de temps pour l'axe X (Anne-Mois)\n",
    "    df_sales_trend['YearMonth'] = df_sales_trend['Year'].astype(str) + '-' + df_sales_trend['Month'].astype(str).str.zfill(2)\n",
    "\n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(x='YearMonth', y='MonthlySales', data=df_sales_trend, marker='o')\n",
    "    plt.title('Tendance des Ventes Mensuelles (SalesAmount)', fontsize=16)\n",
    "    plt.xlabel('Mois', fontsize=12)\n",
    "    plt.ylabel('Revenus Totaux ($)', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6fd166",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Analyse 2 : Performance Commerciale\n",
    "\n",
    "**Objectif :** Identifier les employs ayant gnr le plus de revenus pour rcompenser les meilleurs performeurs et analyser leurs mthodes de vente.\n",
    "\n",
    "### **Conclusion :**\n",
    "Le classement des ventes montre une nette domination de certains employs, notamment **Margaret Peacock** et **Janet Leverling**. La direction devrait examiner de prs leurs zones gographiques et leurs mthodes pour reproduire ce succs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401073f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if conn:\n",
    "    # Requte SQL pour agrger les ventes par employ\n",
    "    sql_query_top_employees = \"\"\"\n",
    "    SELECT\n",
    "        de.FirstName + ' ' + de.LastName AS EmployeeName,\n",
    "        SUM(fs.SalesAmount) AS TotalSales\n",
    "    FROM FactSales fs\n",
    "    JOIN DimEmployees de ON fs.EmployeeID = de.EmployeeKey -- Utilisation de EmployeeID de FactSales et EmployeeKey de DimEmployees\n",
    "    GROUP BY de.FirstName, de.LastName\n",
    "    ORDER BY TotalSales DESC;\n",
    "    \"\"\"\n",
    "\n",
    "    df_top_employees = pd.read_sql(sql_query_top_employees, conn)\n",
    "\n",
    "    # --- Visualisation (Graphique  Barres Horizontal) ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='TotalSales', y='EmployeeName', data=df_top_employees, palette='viridis')\n",
    "    plt.title('Performance des Employs (Ventes Totales)')\n",
    "    plt.xlabel('Montant des Ventes (USD)')\n",
    "    plt.ylabel('Employ')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nObservation : Top 3 des employs : {df_top_employees.iloc[0]['EmployeeName']}, {df_top_employees.iloc[1]['EmployeeName']}, {df_top_employees.iloc[2]['EmployeeName']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0931cd",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Analyse 3 : Distribution du Volume de Ventes par Catgorie\n",
    "\n",
    "**Objectif :** Comprendre sur quelles catgories de produits se concentre la majorit des commandes et des articles vendus (`OrderQuantity`).\n",
    "\n",
    "### **Conclusion :**\n",
    "Le diagramme circulaire confirme que quelques catgories (typiquement **Seafood** et **Dairy Products**) reprsentent une part disproportionne du volume total. Une stratgie marketing et d'approvisionnement devrait se concentrer davantage sur ces segments cls pour maximiser le potentiel de croissance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc478e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if conn:\n",
    "    # Requte: Distribution du volume de commandes par Catgorie de Produit\n",
    "    query_category_volume = \"\"\"\n",
    "    SELECT \n",
    "        DP.CategoryName,\n",
    "        SUM(FS.OrderQuantity) AS TotalQuantity\n",
    "    FROM \n",
    "        FactSales FS\n",
    "    JOIN \n",
    "        DimProducts DP ON FS.ProductID = DP.ProductKey\n",
    "    GROUP BY \n",
    "        DP.CategoryName\n",
    "    ORDER BY \n",
    "        TotalQuantity DESC;\n",
    "    \"\"\"\n",
    "\n",
    "    df_category_volume = pd.read_sql(query_category_volume, conn)\n",
    "    \n",
    "    # Visualisation (Diagramme Circulaire)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    # Utilisation de autopct='%1.1f%%' pour afficher le pourcentage\n",
    "    plt.pie(\n",
    "        df_category_volume['TotalQuantity'], \n",
    "        labels=df_category_volume['CategoryName'], \n",
    "        autopct='%1.1f%%', \n",
    "        startangle=90, \n",
    "        wedgeprops={'edgecolor': 'black'}\n",
    "    )\n",
    "    plt.title('Distribution du Volume de Commandes par Catgorie', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "if conn:\n",
    "    # Requte: Vrification du nombre total de transactions et du CA total\n",
    "    query_verification = \"\"\"\n",
    "    SELECT\n",
    "        COUNT(DISTINCT OrderID) AS TotalOrders,\n",
    "        COUNT(SalesAmount) AS TotalOrderDetails,\n",
    "        SUM(SalesAmount) AS TotalRevenue,\n",
    "        MIN(OrderDateKey) AS FirstOrderDateKey,\n",
    "        MAX(OrderDateKey) AS LastOrderDateKey\n",
    "    FROM\n",
    "        FactSales;\n",
    "    \"\"\"\n",
    "\n",
    "    df_verification = pd.read_sql(query_verification, conn)\n",
    "    \n",
    "    # Affichage des rsultats\n",
    "    print(\"\\n--- Donnes de FactSales aprs Consolidation (SQL + Access) ---\")\n",
    "    print(df_verification.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a90f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# 1. Connexion  TON Data Warehouse actuel\n",
    "engine = create_engine(f'mssql+pyodbc://DESKTOP-F8N2M8C\\\\SQLEXPRESS/NorthwindDW?driver=ODBC+Driver+17+for+SQL+Server')\n",
    "\n",
    "# 2. Lecture des tables telles qu'elles sont dans ta base\n",
    "df_cust = pd.read_sql(\"SELECT * FROM DimCustomers\", engine)\n",
    "df_emp = pd.read_sql(\"SELECT * FROM DimEmployees\", engine)\n",
    "df_fact = pd.read_sql(\"SELECT * FROM FactSales\", engine)\n",
    "df_time = pd.read_sql(\"SELECT * FROM DimDate\", engine)\n",
    "\n",
    "# 3. TRADUCTION pour correspondre au code de ton ami (On renomme tout ici)\n",
    "# --- Clients ---\n",
    "customers = df_cust.rename(columns={\n",
    "    'CustomerKey': 'CustomerID',\n",
    "    'CustomerCompanyName': 'Company',\n",
    "    'CustomerContactName': 'ContactName',\n",
    "    'CustomerCountry': 'Country'\n",
    "})\n",
    "\n",
    "# --- Employs ---\n",
    "employees = df_emp.copy()\n",
    "employees['FullName'] = employees['FirstName'] + ' ' + employees['LastName']\n",
    "employees = employees.rename(columns={'EmployeeKey': 'EmployeeID'})\n",
    "\n",
    "# --- Temps ---\n",
    "time_dim = df_time.rename(columns={'DateKey': 'DateKey'})\n",
    "\n",
    "# --- Faits (La table Orders de ton ami) ---\n",
    "orders = df_fact.rename(columns={\n",
    "    'OrderDateKey': 'OrderDateKey',\n",
    "    'ShippedDateKey': 'ShippedDateKey'\n",
    "})\n",
    "\n",
    "# IMPORTANT : Appliquer la logique \"1011900\" de ton ami pour les non-livrs\n",
    "# Si la date de livraison est vide (NaN), on met le code 1011900\n",
    "orders['ShippedDateKey'] = orders['ShippedDateKey'].fillna(1011900).astype(int)\n",
    "\n",
    "print(\" Traduction termine. Les variables 'customers', 'employees', 'orders' et 'time_dim' sont prtes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90798b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Calcul du rsum des livraisons (Logique 1011900)\n",
    "shipped_count = (orders['ShippedDateKey'] != 1011900).sum()\n",
    "not_shipped_count = (orders['ShippedDateKey'] == 1011900).sum()\n",
    "\n",
    "print(f\" Rsum : {shipped_count} commandes livres / {not_shipped_count} commandes en attente.\")\n",
    "\n",
    "# 2. Performance des employs (Top 5 Livraisons)\n",
    "shipped_by_emp = orders[orders['ShippedDateKey'] != 1011900].groupby('EmployeeID').size().reset_index(name='Total')\n",
    "shipped_by_emp = shipped_by_emp.merge(employees[['EmployeeID', 'FullName']], on='EmployeeID')\n",
    "shipped_by_emp = shipped_by_emp.sort_values('Total', ascending=False)\n",
    "\n",
    "# 3. Visualisation simple\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=shipped_by_emp.head(5), x='FullName', y='Total', palette='viridis')\n",
    "plt.title('Top 5 Employs par Commandes Livres')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 4. Commandes non livres par Pays (Pour identifier les retards)\n",
    "not_shipped_geo = orders[orders['ShippedDateKey'] == 1011900].merge(customers[['CustomerID', 'Country']], on='CustomerID')\n",
    "not_shipped_geo = not_shipped_geo.groupby('Country').size().reset_index(name='EnAttente').sort_values('EnAttente', ascending=False)\n",
    "\n",
    "print(\"\\n Top pays avec commandes en attente :\")\n",
    "print(not_shipped_geo.head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a6bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Prparation des donnes pour le camembert (Global)\n",
    "labels = ['Livr', 'En Attente']\n",
    "sizes = [shipped_count, not_shipped_count]\n",
    "colors = ['#66b3ff','#ff9999']\n",
    "\n",
    "# 2. Performance par Employ (Livr vs Non-Livr)\n",
    "# On cre une vue pivot pour comparer\n",
    "emp_perf = orders.groupby(['EmployeeID', orders['ShippedDateKey'] == 1011900]).size().unstack(fill_value=0)\n",
    "emp_perf.columns = ['Livr', 'En_Attente']\n",
    "emp_perf = emp_perf.merge(employees[['EmployeeID', 'FullName']], on='EmployeeID')\n",
    "\n",
    "# --- AFFICHAGE ---\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Graphique 1 : Rpartition Globale\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, colors=colors, explode=(0.1, 0))\n",
    "plt.title('tat Global des Commandes')\n",
    "\n",
    "# Graphique 2 : Retards par Pays (Ton TOP 5)\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.barplot(data=not_shipped_geo.head(5), x='EnAttente', y='Country', palette='Reds_r')\n",
    "plt.title('Top 5 des Pays en Retard')\n",
    "\n",
    "# Graphique 3 : Performance des Employs\n",
    "plt.subplot(2, 1, 2)\n",
    "emp_perf_melted = emp_perf.melt(id_vars='FullName', value_vars=['Livr', 'En_Attente'])\n",
    "sns.barplot(data=emp_perf_melted, x='FullName', y='value', hue='variable', palette={'Livr': 'skyblue', 'En_Attente': 'salmon'})\n",
    "plt.title('Productivit des Employs : Livraisons vs Retards')\n",
    "plt.xticks(rotation=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4862d7ad",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Fin de l'Analyse\n",
    "\n",
    "La connexion au Data Warehouse a t ferme. Tous les rsultats analytiques sont dsormais affichs ci-dessus et prts  tre intgrs au Rapport Final de Projet (PDF) et  la vido de prsentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad8a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "if conn:\n",
    "    conn.close()\n",
    "    print(\"\\nConnexion SQL Server ferme.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}